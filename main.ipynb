{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f824b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment variables\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "api_key = os.getenv(\"DEEPSEEK_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import regex as re\n",
    "import json\n",
    "import json_repair\n",
    "import pickle\n",
    "import uuid\n",
    "\n",
    "from prompt import *\n",
    "import utils\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"deepseek-chat\",\n",
    "    api_key=api_key,\n",
    "    base_url=\"https://api.deepseek.com/v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57170c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and chunk text\n",
    "filename = \"symposium_short.txt\"\n",
    "loader = TextLoader(filename)\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=1200)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embedding function initialization\n",
    "embedding_function = OllamaEmbeddings(\n",
    "    model=\"bge-m3:567m\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2590c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction\n",
    "tuple_delimiter = PROMPTS[\"DEFAULT_TUPLE_DELIMITER\"] \n",
    "record_delimiter = PROMPTS[\"DEFAULT_RECORD_DELIMITER\"]  \n",
    "completion_delimiter = PROMPTS[\"DEFAULT_COMPLETION_DELIMITER\"] \n",
    "\n",
    "prompts = []\n",
    "for chunk in chunks:\n",
    "    prompt_template = PROMPTS[\"entity_extraction\"]\n",
    "    examples_template = PROMPTS[\"entity_extraction_examples\"]\n",
    "    examples = \"\\n\".join(examples_template).format(\n",
    "        tuple_delimiter=tuple_delimiter,\n",
    "        record_delimiter=record_delimiter,\n",
    "        completion_delimiter=completion_delimiter,\n",
    "    )\n",
    "    prompt = prompt_template.format(\n",
    "        language=\"English\",\n",
    "        entity_types=PROMPTS[\"DEFAULT_ENTITY_TYPES\"],\n",
    "        tuple_delimiter=tuple_delimiter,\n",
    "        record_delimiter=record_delimiter,\n",
    "        completion_delimiter=completion_delimiter,\n",
    "        examples=examples,\n",
    "        input_text=chunk.page_content\n",
    "    )\n",
    "    prompts.append(prompt)\n",
    "\n",
    "cache_file = f\"{filename} responses_cache.pkl\"\n",
    "\n",
    "if os.path.exists(cache_file):\n",
    "    print(\"Loading responses from cache...\")\n",
    "    with open(cache_file, 'rb') as f:\n",
    "        responses = pickle.load(f)\n",
    "else:\n",
    "    print(\"Fetching responses from LLM and caching...\")\n",
    "    responses = llm.batch(prompts)\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump(responses, f)\n",
    "print(\"Responses loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1766022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing extraction output\n",
    "graph_nodes = defaultdict(lambda: {\"descriptions\": [], \"type\": \"\", \"chunk_ids\": []})\n",
    "graph_edges = defaultdict(lambda: {\"descriptions\": [], \"keywords\": [], \"weight\": 1.0, \"chunk_ids\": []})\n",
    "\n",
    "for chunk_id, response in enumerate(responses):\n",
    "    response = response.content\n",
    "    \n",
    "    records = utils.split_string_by_multi_markers(response, [record_delimiter, completion_delimiter])\n",
    "\n",
    "    for record in records:\n",
    "        record = re.search(r\"\\((.*)\\)\", record)\n",
    "        if record:\n",
    "            continue\n",
    "        record = record.group(1)\n",
    "        record_attributes = utils.split_string_by_multi_markers(record, tuple_delimiter)\n",
    "\n",
    "        # Parse entity\n",
    "        entity_data = utils._handle_single_entity_extraction(record_attributes, llm)\n",
    "        if entity_data:\n",
    "            name = entity_data[\"entity_name\"]\n",
    "            type = entity_data[\"entity_type\"]\n",
    "            description = entity_data[\"description\"]\n",
    "\n",
    "            # Deduplication by name\n",
    "            graph_nodes[name][\"descriptions\"].append(description)\n",
    "            graph_nodes[name][\"chunk_ids\"].append(chunk_id)\n",
    "\n",
    "        # Parse relation    \n",
    "        relation_data = utils._handle_single_relation_extraction(record_attributes, llm)\n",
    "        if relation_data:\n",
    "            source = relation_data[\"src_id\"]\n",
    "            target = relation_data[\"tgt_id\"]\n",
    "            weight = relation_data[\"weight\"]\n",
    "            description = relation_data[\"description\"]\n",
    "            keywords = relation_data[\"keywords\"]\n",
    "\n",
    "            key = (source, target)\n",
    "            graph_edges[key][\"keywords\"].append(keywords)\n",
    "            graph_edges[key][\"weight\"] = weight\n",
    "\n",
    "            # Deduplication by key\n",
    "            graph_edges[key][\"descriptions\"].append(description)\n",
    "            graph_edges[key][\"chunk_ids\"].append(chunk_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576c13ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduplication\n",
    "processed_graph_cache_file = f\"{filename} processed_graph.pkl\"\n",
    "\n",
    "temp_entities_vectorstore = Chroma(\n",
    "        collection_name=\"temp_entities\",\n",
    "        embedding_function=embedding_function,\n",
    "        persist_directory=\"./graph\",\n",
    "        collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "\n",
    "if os.path.exists(processed_graph_cache_file):\n",
    "    print(\"Loading processed graph nodes, edges, and summaries from cache...\")\n",
    "    with open(processed_graph_cache_file, 'rb') as f:\n",
    "        cached_data = pickle.load(f)\n",
    "        graph_nodes = cached_data['graph_nodes']\n",
    "        graph_edges = cached_data['graph_edges']\n",
    "        summaries = cached_data['summaries']\n",
    "    print(\"Success.\")\n",
    "else:\n",
    "    print(\"Deduplicating and summarizing...\")\n",
    "\n",
    "    # Clear the temporary collection\n",
    "    temp_entities_vectorstore.delete_collection()\n",
    "    \n",
    "    temp_entities_vectorstore = Chroma(\n",
    "        collection_name=\"temp_entities\",\n",
    "        embedding_function=embedding_function,\n",
    "        persist_directory=\"./graph\",\n",
    "        collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "        \n",
    "    temp_entities_vectorstore.add_texts([str(key) for key in graph_nodes.keys()])\n",
    "\n",
    "    similarity_threshold = 0.7 # tunable\n",
    "\n",
    "    # Cluster entities by names\n",
    "    entity_clusters = {}\n",
    "    processed_names = set()\n",
    "    for name in graph_nodes.keys():\n",
    "        if name in processed_names:\n",
    "            continue\n",
    "\n",
    "        search_results = temp_entities_vectorstore.similarity_search_with_relevance_scores(name, k=5)\n",
    "        \n",
    "        cluster = [name]\n",
    "        processed_names.add(name)\n",
    "        for document, score in search_results:\n",
    "            variant = document.page_content\n",
    "            if score > similarity_threshold and name != variant:\n",
    "                cluster.append(variant)\n",
    "                processed_names.add(variant)\n",
    "        entity_clusters[name] = sorted(list(set(cluster)))\n",
    "    \n",
    "    name_to_canonical = {}\n",
    "    for name, variants in entity_clusters.items():\n",
    "        canonical = name\n",
    "        for variant in variants:\n",
    "            name_to_canonical[variant] = canonical\n",
    "    \n",
    "\n",
    "    merged_graph_nodes = defaultdict(lambda: {\"canonical\": \"\", \"type\": \"\", \"descriptions\": [], \"chunk_ids\": []})\n",
    "    merged_graph_edges = defaultdict(lambda: {\"descriptions\": [], \"keywords\": [], \"weight\": 1.0, \"count\": 0, \"chunk_ids\": []})\n",
    "\n",
    "    # Merge entities\n",
    "    for name, variants in entity_clusters.items():\n",
    "        descriptions = []\n",
    "        for variant in variants:\n",
    "            if variant in graph_nodes:\n",
    "                descriptions.extend(graph_nodes[variant][\"descriptions\"])\n",
    "                if name != variant:\n",
    "                    print(f\"Merged nodes: {name} <- {variant}\")\n",
    "\n",
    "        new_key = tuple(variants)\n",
    "        merged_graph_nodes[new_key] = {\n",
    "            \"canonical\": name,\n",
    "            \"type\": graph_nodes[name][\"type\"],\n",
    "            \"descriptions\": descriptions,\n",
    "            \"chunk_ids\": graph_nodes[name][\"chunk_ids\"]\n",
    "        }\n",
    "    \n",
    "    # Reconnect relations\n",
    "    for key, data in graph_edges.items():\n",
    "        source, target = key\n",
    "        canonical_source = name_to_canonical.get(source)\n",
    "        canonical_target = name_to_canonical.get(target)\n",
    "\n",
    "        if canonical_source and canonical_target:\n",
    "            new_key = (canonical_source, canonical_target)\n",
    "            merged_graph_edges[new_key][\"descriptions\"].extend(data[\"descriptions\"]) \n",
    "            merged_graph_edges[new_key][\"keywords\"].extend(data[\"keywords\"])\n",
    "            merged_graph_edges[new_key][\"weight\"] += data[\"weight\"] \n",
    "            merged_graph_edges[new_key][\"count\"] += 1 \n",
    "            merged_graph_edges[new_key][\"chunk_ids\"].extend(data[\"chunk_ids\"])\n",
    "    \n",
    "    for key in merged_graph_edges:\n",
    "        if merged_graph_edges[new_key][\"count\"] > 1: \n",
    "            merged_graph_edges[new_key][\"weight\"] /= merged_graph_edges[new_key][\"count\"] \n",
    "\n",
    "    graph_nodes = merged_graph_nodes\n",
    "    graph_edges = merged_graph_edges\n",
    "    \n",
    "    print(f\"Number of nodes: {len(graph_nodes)}\")\n",
    "    print(f\"Number of edges: {len(graph_nodes)}\")\n",
    "\n",
    "    # Summarization\n",
    "    entity_or_relation_names = []\n",
    "    entity_or_relation_descriptions = []\n",
    "\n",
    "    for name, data in graph_nodes.items():\n",
    "        entity_or_relation_names.append(\", \".join(name))\n",
    "        entity_or_relation_descriptions.append(\" \".join(data[\"descriptions\"]))\n",
    "    for key, data in graph_edges.items():\n",
    "        entity_or_relation_names.append(\" -> \".join(key))\n",
    "        entity_or_relation_descriptions.append(\" \".join(data[\"descriptions\"]))\n",
    "\n",
    "    summaries = utils._handle_entity_relation_summary(entity_or_relation_names, entity_or_relation_descriptions, filename, llm)\n",
    "\n",
    "    with open(processed_graph_cache_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'graph_nodes': dict(graph_nodes),\n",
    "            'graph_edges': dict(graph_edges),\n",
    "            'summaries': summaries\n",
    "        }, f)\n",
    "    print(\"Processed graph data cached successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd644fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage: Networkx and FAISS\n",
    "graph_storage_file = f\"{filename} graph_storage.pkl\"\n",
    "if os.path.exists(graph_storage_file):\n",
    "    print(\"Loading graph from cache...\")\n",
    "    with open(graph_storage_file, 'rb') as f:\n",
    "        G = pickle.load(f)\n",
    "    print(\"Success.\")\n",
    "\n",
    "else:\n",
    "    print(\"Generating graph...\")\n",
    "    G = nx.DiGraph()\n",
    "    nodes = []\n",
    "    edges = []\n",
    "    summary_index = 0\n",
    "\n",
    "    for name_tuple, data in graph_nodes.items():\n",
    "        summary = summaries[summary_index].content\n",
    "        canonical = data[\"canonical\"]\n",
    "\n",
    "        node_id = str(uuid.uuid4()) # TODO\n",
    "\n",
    "        # Add node to graph\n",
    "        G.add_node(\n",
    "                canonical, # Name as node for now\n",
    "                graph_id=canonical, \n",
    "                type=data['type'], \n",
    "                description=summary,\n",
    "                chunk_ids=data[\"chunk_ids\"]\n",
    "            )\n",
    "        \n",
    "        print(f\"Adding node: {canonical} with summary: {summary}...\")\n",
    "        \n",
    "        # Prepare node document\n",
    "        nodes.append(\n",
    "            Document(\n",
    "                page_content=summary, \n",
    "                metadata={\n",
    "                    \"graph_id\": canonical, # Name as id for now\n",
    "                    \"type\": data[\"type\"],\n",
    "                    \"value\": summary,\n",
    "                    \"chunk_ids\": \" \".join([str(id) for id in data[\"chunk_ids\"]])\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    \n",
    "        summary_index += 1\n",
    "\n",
    "    for key, data in graph_edges.items():\n",
    "        summary = summaries[summary_index].content\n",
    "        source, target = key\n",
    "\n",
    "        edge_id = str(uuid.uuid4())\n",
    "\n",
    "        # Add edge to graph\n",
    "        G.add_edge(\n",
    "                source, \n",
    "                target, \n",
    "                graph_id=edge_id, \n",
    "                description=summary, \n",
    "                keywords=data['keywords'], \n",
    "                weight=float(data['weight']),\n",
    "                chunk_ids=data[\"chunk_ids\"]\n",
    "                )\n",
    "        \n",
    "        print(f\"Adding edge: ({source}, {target}) with summary: {summary}...\")\n",
    "        \n",
    "        # Prepare edge document\n",
    "        edges.append(\n",
    "            Document(\n",
    "                page_content=summary,\n",
    "                metadata={\n",
    "                    \"graph_id\": edge_id, \n",
    "                    \"value\": summary,\n",
    "                    \"source\": source, \n",
    "                    \"target\": target,\n",
    "                    \"weight\": data[\"weight\"],\n",
    "                    \"chunk_ids\": \" \".join([str(id) for id in data[\"chunk_ids\"]])\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        summary_index += 1\n",
    "\n",
    "    print(\"Adding nodes and edges to vectorstores...\")\n",
    "\n",
    "    # Populate vectorstore\n",
    "    entities_vectorstore = FAISS.from_documents(documents=nodes, embedding=embedding_function)\n",
    "    relations_vectorstore = FAISS.from_documents(documents=edges, embedding=embedding_function)\n",
    "\n",
    "    print(\"Caching graph...\")\n",
    "    with open(f\"{filename} graph.pkl\", \"wb\") as f:\n",
    "        pickle.dump(G, f)\n",
    "    print(\"Success.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e082c12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph visualization\n",
    "plt.figure(figsize=(50, 50))\n",
    "\n",
    "pos = nx.spring_layout(G, k=0.01, iterations=1000, seed=1172)\n",
    "nx.draw(G, with_labels=True, font_size=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a72d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query keywords extraction\n",
    "\n",
    "# TODO: query from input\n",
    "query = \"Compare and contrast the different theories about love mentioned in the text, illustrating how each of them helps conveying the main philosophical thought.\"\n",
    "\n",
    "history = \"\" # TODO\n",
    "\n",
    "prompt_template = PROMPTS[\"keywords_extraction\"]\n",
    "examples = PROMPTS[\"keywords_extraction_examples\"]\n",
    "prompt = prompt_template.format(\n",
    "    examples=examples,\n",
    "    history=history,\n",
    "    query=query\n",
    ")\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "\n",
    "try:\n",
    "    keywords_data = json_repair.loads(response.content)\n",
    "    if not keywords_data:\n",
    "        print(\"No JSON-like structure found in the LLM response.\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"JSON parsing error: {e}\")\n",
    "    print(f\"LLM response: {keywords_data}\")\n",
    "\n",
    "hl_keywords = keywords_data.get(\"high_level_keywords\", [])\n",
    "ll_keywords = keywords_data.get(\"low_level_keywords\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db5e36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keyword matching\n",
    "# TODO: mode from input\n",
    "query_mode = \"\"\n",
    "\n",
    "local_entities = []\n",
    "local_relations = []\n",
    "global_entities = []\n",
    "global_relations = []\n",
    "\n",
    "if query_mode == \"local\":\n",
    "    for ll_keyword in ll_keywords:\n",
    "        local_entities.extend(entities_vectorstore.similarity_search(ll_keyword, k=3))\n",
    "        graph_ids = [entity.metadata.get(\"graph_id\") for entity in local_entities if entity.metadata.get(\"graph_id\") is not None]\n",
    "        \n",
    "        for graph_id in graph_ids:\n",
    "            for neighbor in G.neighbors(graph_id):\n",
    "                edge_data = G.get_edge_data(graph_id, neighbor)\n",
    "                if edge_data:\n",
    "                    local_relations.append(edge_data)elif query_mode == \"global\":\n",
    "    \n",
    "    for hl_keyword in hl_keywords:\n",
    "        global_relations.extend(relations_vectorstore.similarity_search(hl_keyword, k=3))\n",
    "        graph_ids = [relation.metadata.get(\"graph_id\") for relation in global_relations]\n",
    "\n",
    "        for relation in global_relations:\n",
    "            source = relation.metadata.get(\"source\")\n",
    "            target = relation.metadata.get(\"target\")\n",
    "            if source and source in G.nodes:\n",
    "                global_entities.append(G.nodes[source])\n",
    "            if target and target in G.nodes:\n",
    "                global_entities.append(G.nodes[target])\n",
    "\n",
    "else:\n",
    "    for ll_keyword in ll_keywords:\n",
    "        local_entities.extend(entities_vectorstore.similarity_search(ll_keyword, k=3))\n",
    "        graph_ids = [entity.metadata.get(\"graph_id\") for entity in local_entities if entity.metadata.get(\"graph_id\") is not None]\n",
    "        \n",
    "        for graph_id in graph_ids:\n",
    "            for neighbor in G.neighbors(graph_id):\n",
    "                edge_data = G.get_edge_data(graph_id, neighbor)\n",
    "                if edge_data:\n",
    "                    local_relations.append(edge_data)\n",
    "    \n",
    "    for hl_keyword in hl_keywords:\n",
    "        global_relations.extend(relations_vectorstore.similarity_search(hl_keyword, k=3))\n",
    "        graph_ids = [relation.metadata.get(\"graph_id\") for relation in global_relations]\n",
    "\n",
    "        for relation in global_relations:\n",
    "            source = relation.metadata.get(\"source\")\n",
    "            target = relation.metadata.get(\"target\")\n",
    "            if source and source in G.nodes:\n",
    "                global_entities.append(G.nodes[source])\n",
    "            if target and target in G.nodes:\n",
    "                global_entities.append(G.nodes[target])\n",
    "\n",
    "# Make all four lists type <class 'dict'>\n",
    "local_entities = [relation.metadata for relation in local_entities]\n",
    "global_relations = [entity.metadata for entity in global_relations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a927b466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round-robin merge \n",
    "# borrowed from https://github.com/HKUDS/LightRAG/blob/main/lightrag/operate.py\n",
    "chunk_ids = set()\n",
    "\n",
    "# Round-robin merge entities\n",
    "final_entities = []\n",
    "seen_ids = set()\n",
    "\n",
    "max_len = max(len(local_entities), len(global_entities))\n",
    "for i in range(max_len):\n",
    "    # First from local\n",
    "    if i < len(local_entities):\n",
    "        entity = local_entities[i]\n",
    "        entity_id = entity.get(\"graph_id\")\n",
    "        if entity_id and entity_id not in seen_ids:\n",
    "            final_entities.append(entity)\n",
    "            seen_ids.add(entity_id)\n",
    "\n",
    "            chunk_ids.update(entity[\"chunk_ids\"])\n",
    "    # Then from global\n",
    "    if i < len(global_entities):\n",
    "        entity = global_entities[i]\n",
    "        entity_id = entity.get(\"graph_id\")\n",
    "        if entity_id and entity_id not in seen_ids:\n",
    "            final_entities.append(entity)\n",
    "            seen_ids.add(entity_id)\n",
    "\n",
    "            chunk_ids.update(entity[\"chunk_ids\"])\n",
    "\n",
    "# Round-robin merge relations\n",
    "final_relations = []\n",
    "seen_ids = set()\n",
    "\n",
    "max_len = max(len(local_relations), len(global_relations))\n",
    "for i in range(max_len):\n",
    "    # First from local\n",
    "    if i < len(local_relations):\n",
    "        relation = local_relations[i]\n",
    "        # Build relation unique identifier\n",
    "        relation_id = relation.get(\"graph_id\")\n",
    "        if relation_id not in seen_ids:\n",
    "            final_relations.append(relation)\n",
    "            seen_ids.add(relation_id)\n",
    "\n",
    "            chunk_ids.update(relation[\"chunk_ids\"])\n",
    "\n",
    "    # Then from global\n",
    "    if i < len(global_relations):\n",
    "        relation = global_relations[i]\n",
    "        # Build relation unique identifier\n",
    "        relation_id = relation.get(\"graph_id\")\n",
    "        if relation_id not in seen_ids:\n",
    "            final_relations.append(relation)\n",
    "            seen_ids.add(relation_id)\n",
    "\n",
    "            chunk_ids.update(relation[\"chunk_ids\"])\n",
    "\n",
    "# Generate entities context\n",
    "entities_context = []\n",
    "for i, n in enumerate(final_entities):\n",
    "\n",
    "    # Get file path from node data\n",
    "    #file_path = n.get(\"file_path\", \"unknown_source\")\n",
    "\n",
    "    entities_context.append(\n",
    "        {\n",
    "            \"id\": i + 1,\n",
    "            \"entity\": n[\"graph_id\"],\n",
    "            \"type\": n.get(\"type\", \"UNKNOWN\"),\n",
    "            \"description\": n.get(\"value\", \"UNKNOWN\"),\n",
    "            #\"file_path\": file_path,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Generate relations context\n",
    "relations_context = []\n",
    "for i, e in enumerate(final_relations):\n",
    "\n",
    "    # Get file path from edge data\n",
    "    #file_path = e.get(\"file_path\", \"unknown_source\")\n",
    "\n",
    "    relations_context.append(\n",
    "        {\n",
    "            \"id\": i + 1,\n",
    "            \"entity1\": e.get(\"source\"),\n",
    "            \"entity2\": e.get(\"target\"),\n",
    "            \"description\": e.get(\"value\", \"UNKNOWN\"),\n",
    "            #\"file_path\": file_path,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Generate chunks context\n",
    "merged_chunks = []\n",
    "chunk_ids = [int(id) for id in chunk_ids] #fix original logic later\n",
    "for chunk_id in chunk_ids:\n",
    "    merged_chunks.append(\n",
    "                        {\n",
    "                            \"content\": chunks[chunk_id],\n",
    "                            #\"file_path\": chunk.get(\"file_path\", \"unknown_source\"),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "context_data = {\n",
    "    \"entities\": entities_context,\n",
    "    \"relations\": relations_context,\n",
    "    \"chunks\": merged_chunks\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023a84f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build context from context data\n",
    "# borrowed from https://github.com/HKUDS/LightRAG/blob/main/lightrag/operate.py\n",
    "if \"entities\" in context_data and context_data[\"entities\"]:\n",
    "        entities = context_data[\"entities\"]\n",
    "        entities_str = f\"--- Entities ---\\n{json.dumps(entities, indent=2, ensure_ascii=False)}\\n\"\n",
    "\n",
    "if \"relations\" in context_data and context_data[\"relations\"]:\n",
    "    relations = context_data[\"relations\"]\n",
    "    relations_str = f\"--- Relations ---\\n{json.dumps(relations, indent=2, ensure_ascii=False)}\\n\"\n",
    "\n",
    "if \"chunks\" in context_data and context_data[\"chunks\"]:\n",
    "    chunks = context_data[\"chunks\"]\n",
    "    chunks_str_list = []\n",
    "    for chunk in chunks:\n",
    "        chunks_str_list.append(f\"Text: {chunk}\")\n",
    "    chunks_str = f\"--- Text Chunks ---\\n\" + \"\\n\\n\".join(chunks_str_list)\n",
    "\n",
    "context_str = f\"{entities_str}{relations_str}{chunks_str}\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12639018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final query\n",
    "prompt_template = PROMPTS[\"rag_response\"]\n",
    "final_query = prompt_template.format(\n",
    "    history=\"\",\n",
    "    context_data=context_str,\n",
    "    response_type=\"\",\n",
    "    user_prompt=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4c0f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final generation\n",
    "answer = llm.invoke(final_query)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rags",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
